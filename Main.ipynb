{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T05:25:32.084983Z",
     "iopub.status.busy": "2024-11-10T05:25:32.084121Z",
     "iopub.status.idle": "2024-11-10T05:25:35.762792Z",
     "shell.execute_reply": "2024-11-10T05:25:35.762003Z",
     "shell.execute_reply.started": "2024-11-10T05:25:32.084935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Modifikasi\n",
    "import warnings\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "#Perhitungan\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Imputasi\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV, GroupKFold,KFold, TimeSeriesSplit   \n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc,roc_auc_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, RepeatVector, TimeDistributed\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Feature Importance\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-10T05:25:36.156095Z",
     "iopub.status.busy": "2024-11-10T05:25:36.155218Z",
     "iopub.status.idle": "2024-11-10T05:25:36.531229Z",
     "shell.execute_reply": "2024-11-10T05:25:36.530217Z",
     "shell.execute_reply.started": "2024-11-10T05:25:36.156054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('raw_dataset/Train.csv')\n",
    "test = pd.read_csv('raw_dataset/Test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\booma\\AppData\\Local\\Temp\\ipykernel_31248\\2488958140.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  train['Date'] = pd.to_datetime(train['Date'], dayfirst=True, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "train['Date'] = pd.to_datetime(train['Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "Date = train.copy()\n",
    "Date['Date'] = pd.to_datetime(Date['Date'])\n",
    "\n",
    "\n",
    "# 3. Menetapkan kolom Date sebagai index\n",
    "train.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\booma\\AppData\\Local\\Temp\\ipykernel_31248\\1334361193.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  test['Date'] = pd.to_datetime(test['Date'], dayfirst=True, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "test['Date'] = pd.to_datetime(test['Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "Date_test = test.copy()\n",
    "Date_test['Date'] = pd.to_datetime(Date['Date'])\n",
    "\n",
    "\n",
    "# 3. Menetapkan kolom Date sebagai index\n",
    "test.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = train.copy()\n",
    "# df.drop(columns=['ID_Zindi','ID'],inplace=True)\n",
    "# test.drop(columns=['ID_Zindi','ID'],inplace=True)\n",
    "\n",
    "# def impute_missing_values(df, cols_to_impute, drop_cols=['LAT', 'LON'], n_estimators=100, random_state=42):\n",
    "#     for col in cols_to_impute:\n",
    "#         if df[col].isna().sum() > 0:  # Cek apakah ada nilai NaN pada kolom\n",
    "#             non_missing_data = df[df[col].notna()]  # Data tanpa nilai NaN untuk training\n",
    "#             X_train = non_missing_data.drop(columns=[col] + drop_cols)  # Fitur training tanpa kolom target\n",
    "#             y_train = non_missing_data[col]  # Target untuk training\n",
    "            \n",
    "#             # Inisiasi Random Forest Regressor dan training\n",
    "#             rf_imputer = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n",
    "#             rf_imputer.fit(X_train, y_train)\n",
    "            \n",
    "#             # Melakukan prediksi untuk mengisi nilai NaN\n",
    "#             X_pred = df[df[col].isna()].drop(columns=[col] + drop_cols)\n",
    "#             df.loc[df[col].isna(), col] = rf_imputer.predict(X_pred)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# cols_to_impute_rf = ['AAI', 'CloudFraction','LST', 'NO2_trop', 'NO2_strat', 'NO2_total', 'TropopausePressure']\n",
    "# df = impute_missing_values(df, cols_to_impute_rf)\n",
    "# test = impute_missing_values(test, cols_to_impute_rf)\n",
    "\n",
    "# # Imputasi untuk kolom dengan missing data sedikit (Mean Imputation)\n",
    "# cols_to_impute_mean = ['GT_NO2']\n",
    "# mean_imputer = SimpleImputer(strategy='mean')\n",
    "# df[cols_to_impute_mean] = mean_imputer.fit_transform(df[cols_to_impute_mean])\n",
    "\n",
    "# # Time series imputation using Iterative Imputer (Multiple Imputation)\n",
    "# time_series_cols = ['Precipitation']\n",
    "# time_series_imputer = IterativeImputer(random_state=42)\n",
    "# df[time_series_cols] = time_series_imputer.fit_transform(df[time_series_cols])\n",
    "# test[time_series_cols] = time_series_imputer.fit_transform(test[time_series_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('train_imputed_with_rf_regressor.csv', index=False)\n",
    "# test.to_csv('test_imputed_with_rf_regressor.csv', index=False)\n",
    "\n",
    "data = pd.read_csv('final_dataset/train_imputed_with_rf_regressor.csv')\n",
    "dtest = pd.read_csv('final_dataset/test_imputed_with_rf_regressor.csv')\n",
    "\n",
    "data.drop(['LAT','LON'],axis=1,inplace=True)\n",
    "dtest.drop(['LAT','LON'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opsional: Cek pola musiman dengan boxplot\n",
    "data[\"month\"] = train.index.month\n",
    "data[\"year\"] = train.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal Encoding Bulan ``\n",
    "data['month_Sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "data['month_Cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "\n",
    "# Fitur Elapsed months (pastikan tahun dimulai dari min tahun di data)\n",
    "min_year = data['year'].min()\n",
    "data['Elapsed_months'] = (data['year'] - min_year) * 12 + data['month']\n",
    "\n",
    "# Quarter & Semester\n",
    "data['Quarter'] = ((data['month'] - 1) // 3 + 1).astype(int)\n",
    "\n",
    "data['Semester'] = ((data['month'] - 1) // 6 + 1).astype(int)\n",
    "\n",
    "# year-month Identifier (Format YYYYMM)\n",
    "data['yearmonth'] = (data['year'] * 100 + data['month']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "data['Kmeans'] = kmeans.fit_predict(data[['NO2_strat', 'NO2_total', 'NO2_trop']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data['TropopausePressure'] = scaler.fit_transform(data[['TropopausePressure']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misalnya, data berisi kolom 'Precipitation', 'LST', 'AAI'\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Menerapkan transformasi polinomial pada kolom-kolom yang ditentukan\n",
    "poly_features = poly.fit_transform(data[['Precipitation', 'LST', 'AAI']])\n",
    "poly_features_test = poly.fit_transform(dtest[['Precipitation', 'LST', 'AAI']])\n",
    "\n",
    "# Membuat DataFrame baru dengan nama kolom yang sesuai\n",
    "poly_feature_columns = poly.get_feature_names_out(['Precipitation', 'LST', 'AAI'])\n",
    "\n",
    "# Menggabungkan hasilnya dengan data asli (jika perlu)\n",
    "poly_data = pd.DataFrame(poly_features, columns=poly_feature_columns)\n",
    "poly_dtest = pd.DataFrame(poly_features_test, columns=poly_feature_columns)\n",
    "\n",
    "poly_data.drop(['1','Precipitation','LST','AAI',],axis=1,inplace=True)\n",
    "poly_dtest.drop(['1','Precipitation','LST','AAI',],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([data, poly_data], axis=1)\n",
    "result_test = pd.concat([dtest, poly_dtest], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['GT_NO2'] = pd.to_numeric(result['GT_NO2'], errors='coerce')  # Pastikan target numerik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'GT_NO2'  # Nama kolom target\n",
    "cols = [col for col in data.columns if col != target_col] + [target_col]\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, output_length=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - output_length + 1):\n",
    "        X.append(data[i:i+seq_length])  # Ambil input sequence\n",
    "        y.append(data[i+seq_length:i+seq_length+output_length])  # Output beberapa langkah ke depan\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 10.104330172445012\n"
     ]
    }
   ],
   "source": [
    "seq_length = 7\n",
    "output_length = 1  # Prediksi 1 langkah ke depan\n",
    "X, y = create_sequences(data['GT_NO2'].values, seq_length, output_length)\n",
    "\n",
    "# Split data menjadi training dan testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Model SGBRegressor\n",
    "model = HistGradientBoostingRegressor(loss=\"squared_error\", max_iter=100)\n",
    "model.fit(X_train, y_train.ravel())  # `.ravel()` agar sesuai dengan input\n",
    "\n",
    "# Prediksi\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluasi\n",
    "from sklearn.metrics import root_mean_squared_error \n",
    "mse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5968895,
     "sourceId": 9749552,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
